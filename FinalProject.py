# -*- coding: utf-8 -*-
"""NLP final project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18cemMuFjSxxx5mI36oVrCENxM6hgcLZg
"""

!pip install pandas numpy scikit-learn xgboost sentence-transformers transformers accelerate datasets openpyxl pdfplumber python-docx

from google.colab import files
uploaded = files.upload()

filename = list(uploaded.keys())[0]
print(f"Uploaded file: {filename}")

import pandas as pd
import re

def load_any_document(file_path):
    file_path_lower = file_path.lower()
    text_content = ""

    if file_path_lower.endswith((".xlsx", ".xls", ".csv")):
        try:
            if file_path_lower.endswith(".csv"):
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)

            df = df.fillna('')
            print(f"Excel/CSV file loaded with shape {df.shape}. Columns: {list(df.columns)}")
            return "excel", df

        except Exception as e:
            raise ValueError(f"Failed to read spreadsheet file: {e}")

    elif file_path_lower.endswith(".pdf"):
        print("Reading PDF...")
        try:
            import pdfplumber
        except ImportError:
            raise ImportError(" Missing library pdfplumber â€” install using: !pip install pdfplumber")

        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:  # avoid NoneType errors
                    text_content += page_text + "\n"

        print(f"PDF loaded, {len(text_content)} characters extracted.")
        return "text", text_content

    elif file_path_lower.endswith(".docx"):
        print("Reading Word document...")
        try:
            from docx import Document
        except ImportError:
            raise ImportError(" Missing library python-docx â€” install using: !pip install python-docx")

        doc = Document(file_path)
        for para in doc.paragraphs:
            text_content += para.text + "\n"

        print(f" Word file loaded, {len(text_content)} characters extracted.")
        return "text", text_content

    else:
        raise ValueError(f" Unsupported file type: {file_path}\nAllowed: Excel, Word, PDF")

if filename.lower().endswith((".xlsx", ".xls", ".csv", ".pdf", ".docx")):
    file_type, data = load_any_document(filename)
else:
    print(f"Unsupported file type for {filename}. Please upload Excel, PDF, or Word.")

def extract_clean_clauses(file_type, data):
    import re
    from nltk.tokenize import sent_tokenize

    clauses = []
    if file_type == "excel":
        for _, row in data.iterrows():
            row_text = " ".join([str(v) for v in row.values if str(v).strip()])
            clean = preprocess(row_text)
            for s in sent_tokenize(clean):
                if len(s.split()) > 5:
                    clauses.append(s.strip())
    else:
        clean = preprocess(data)
        for s in sent_tokenize(clean):
            if len(s.split()) > 5:
                clauses.append(s.strip())

    print(f" Extracted {len(clauses)} cleaned clauses")
    print(clauses)
    return clauses

labels = None
if file_type == "excel":
    df = data.copy()
    label_col = None
    for col in df.columns:
        if df[col].dtype == object and 1 < df[col].nunique() < len(df):
            label_col = col
            break

    if label_col:
        print(f"Detected label column: {label_col}")

        clean_labels = []
        for _, row in df.iterrows():
            cleaned = preprocess_text(' '.join(map(str, row.values)))
            if len(cleaned.split()) > 10:
                clean_labels.append(str(row[label_col]))
        labels = clean_labels
    else:
        print("No label column found â€” running without labels")
else:
    print("Text file â€” labels not applicable")
    labels = None
clauses = extract_clean_clauses(file_type, data)
if labels:
    print(f"Before alignment â†’ labels: {len(labels)}, clauses: {len(clauses)}")
    min_len = min(len(labels), len(clauses))
    labels = labels[:min_len]
    clauses = clauses[:min_len]
    print(f" After alignment â†’ labels: {len(labels)}, clauses: {len(clauses)}")

clauses = extract_clean_clauses(file_type, data)
print(f" Extracted {len(clauses)} clauses")
if file_type == "excel":
    df = data   # rename correctly

    label_col = None
    for col in df.columns:
        # rules for possible label columns
        if df[col].dtype == object and 1 < df[col].nunique() < len(df):
            label_col = col
            break

    if label_col:
        print(f" Supervised mode ON â€” Detected label column: {label_col}")
        labels = df[label_col].astype(str).tolist()
    else:
        print(" No label column found â€” switching to unsupervised mode")
        labels = None

else:
    print(" Unstructured text detected â†’ unsupervised mode")
    labels = None

from sklearn.feature_extraction.text import TfidfVectorizer

print(" Creating TF-IDF representations for clauses...")

vectorizer = TfidfVectorizer(max_features=15000, stop_words='english')
X = vectorizer.fit_transform(clauses)

print(f"TF-IDF matrix created with shape: {X.shape}")


# LABEL ALIGNMENT - SMART BLOCK MAPPING

if labels:
    if len(labels) != X.shape[0]:
        print(f" Mismatch detected â†’ clauses={X.shape[0]}, labels={len(labels)}")
        print(" Applying smart proportional label mapping...")
        new_labels = []
        for i in range(X.shape[0]):
            mapped_index = i * len(labels) // X.shape[0]
            new_labels.append(labels[mapped_index])
        labels = new_labels
        print(f" Labels aligned: {len(labels)} == {X.shape[0]}")
    else:
        print(" Labels already aligned")



# MODEL TRAINING (SUPERVISED IF LABELS EXIST, OTHERWISE REGRESSION)

import numpy as np
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder

model = None
print(labels)

if labels:
    print("\n MODE: SUPERVISED CLASSIFICATION (TRAIN ONLY)")
    le = LabelEncoder()
    y = le.fit_transform(labels)
    dtrain = xgb.DMatrix(X, label=y)
    params = {
        "objective": "multi:softprob",
        "num_class": len(np.unique(y)),
        "eval_metric": "mlogloss"
    }
    model = xgb.train(params, dtrain, num_boost_round=250)
    print("Classification model trained successfully!")

# ---------------------- REGRESSION FALLBACK ----------------------
if not labels:
    print("\n MODE: UNSUPERVISED REGRESSION (TRAIN ONLY)")
    y_dummy = np.mean(X.toarray(), axis=1)
    synthetic_labels = y_dummy.copy()
    dtrain = xgb.DMatrix(X, label=y_dummy)
    params = {"objective": "reg:squarederror", "eval_metric": "rmse"}
    model = xgb.train(params, dtrain, num_boost_round=250)
    pred_scores = model.predict(dtrain)
    print("Regression model trained successfully!")

print("\n Training Completed. Model Ready for Use.")
print(model)

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix,
    mean_squared_error, mean_absolute_error, r2_score
)
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

print(model)
if model is None:
    print(" No trained model found. Please run training cell first.")

# ------------------------- SUPERVISED EVALUATION -------------------------
elif labels is not None:
    print("\n DETECTED MODE: SUPERVISED CLASSIFICATION")
    print(" Evaluating using true labels (no train/test split)")

    le = LabelEncoder()
    y_true = le.fit_transform(labels)

    dfull = xgb.DMatrix(X)
    preds_proba = model.predict(dfull)
    preds = np.argmax(preds_proba, axis=1)

    acc = accuracy_score(y_true, preds)
    prec = precision_score(y_true, preds, average="weighted", zero_division=0)
    rec = recall_score(y_true, preds, average="weighted", zero_division=0)
    f1 = f1_score(y_true, preds, average="weighted", zero_division=0)

    print("\n SUPERVISED METRICS:")
    print(f"Accuracy :  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(f"F1 Score : {f1:.4f}")

# ------------------------ UNSUPERVISED EVALUATION ------------------------
else:
    print("\n DETECTED MODE: UNSUPERVISED REGRESSION")
    print(" Evaluating using synthetic labels")

    dfull = xgb.DMatrix(X)
    pred_scores_eval = model.predict(dfull)

    rmse = np.sqrt(mean_squared_error(synthetic_labels, pred_scores_eval))
    mae = mean_absolute_error(synthetic_labels, pred_scores_eval)
    r2 = r2_score(synthetic_labels, pred_scores_eval)

    print("\n SYNTHETIC REGRESSION METRICS:")
    print(f"RMSE: {rmse:.6f}")
    print(f"MAE : {mae:.6f}")
    print(f"RÂ²  : {r2:.6f}")

import matplotlib.pyplot as plt

xgb.plot_importance(model, max_num_features=15)
plt.title("Top 15 Important TF-IDF Features Learned by XGBoost")
plt.show()

from sklearn.metrics.pairwise import cosine_similarity
import nltk
nltk.download("punkt")

def retrieve_answers(question, X, clauses, vectorizer, top_k=3, sentence_level=True):
    """Retrieve top matching clauses for a given question."""

    # Convert question into vector form
    q_vec = vectorizer.transform([question])
    similarities = cosine_similarity(q_vec, X).flatten()

    # Sort top matching indices
    top_indices = similarities.argsort()[-top_k:][::-1]

    print(f" QUESTION: {question}")

    for rank, idx in enumerate(top_indices, start=1):
        text = clauses[idx]

        # Extract best sentence (optional)
        if sentence_level:
            sentences = nltk.sent_tokenize(text)
            if len(sentences) > 1:
                sent_vecs = vectorizer.transform(sentences)
                sent_sims = cosine_similarity(q_vec, sent_vecs).flatten()
                best_sentence = sentences[sent_sims.argmax()]
            else:
                best_sentence = text
        else:
            best_sentence = text

        print(f" Top {rank} (score={similarities[idx]:.4f})")
        print(best_sentence)
        print("-" * 60)

import nltk

questions = [
    "Is there any limitation of liability?",
    "What is the warranty period?",
    "What happens if we terminate the contract?",
    "Are there financial penalties or fees?",
]

for q in questions:
    retrieve_answers(q, X, clauses, vectorizer, top_k=3)

!pip install rank_bm25 sentence-transformers

import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, util
import re, nltk
nltk.download("punkt")
nltk.download("punkt_tab")

def explode_sentences_into_snippets(sentences, min_words=4):
    new_units = []
    for sent in sentences:
        parts = re.split(r'[.;:]', sent)  # break into legal atomic units
        for p in parts:
            p = p.strip()
            if len(p.split()) >= min_words:
                new_units.append(p)
    print(f" Expanded {len(sentences)} sentences â†’ {len(new_units)} snippets")
    return new_units

snippets = explode_sentences_into_snippets(clauses)

!pip install rouge_score

# TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1,3))
X_tfidf = vectorizer.fit_transform(snippets)

# BM25
tokenized = [nltk.word_tokenize(s.lower()) for s in snippets]
bm25 = BM25Okapi(tokenized)

# Semantic Embeddings (non-LLM)
embedder = SentenceTransformer("all-MiniLM-L6-v2")
emb_matrix = embedder.encode(snippets, convert_to_tensor=True)

def normalize_scores(scores):
    scores = np.array(scores)
    return (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)

def highlight(text, query):
    q_words = set(query.lower().split())
    words = text.split()
    return " ".join([f"**{w}**" if w.lower() in q_words else w for w in words])

def hybrid_search(question, top_k=5, w_bm25=0.4, w_tfidf=0.3, w_sem=0.3):
    # BM25
    bm25_scores = bm25.get_scores(nltk.word_tokenize(question.lower()))
    bm25_norm = normalize_scores(bm25_scores)

    # TF-IDF
    q_tfidf = vectorizer.transform([question])
    tfidf_scores = cosine_similarity(q_tfidf, X_tfidf).flatten()
    tfidf_norm = normalize_scores(tfidf_scores)

    # Embedding similarity
    q_emb = embedder.encode([question], convert_to_tensor=True)
    sem_scores = util.cos_sim(q_emb, emb_matrix).cpu().numpy().flatten()
    sem_norm = normalize_scores(sem_scores)

    # Hybrid weighted
    final_scores = w_bm25*bm25_norm + w_tfidf*tfidf_norm + w_sem*sem_norm
    best_idx = final_scores.argsort()[-top_k:][::-1]

    print(f"QUESTION: {question}")

    for rank, idx in enumerate(best_idx, 1):
        score = final_scores[idx]
        print(f" Match {rank} | Score={score:.3f}")
        print(highlight(snippets[idx], question))
        print("-"*70)

questions = [
    "Is my liability limited or unlimited under this agreement?",
    "What is the maximum amount I can be held responsible for?",
    "Are there exceptions where liability becomes uncapped (e.g., fraud, willful misconduct, IP infringement, data breach)?",
    "Are indirect, incidental, punitive, or consequential damages excluded?"
]

for q in questions:
    hybrid_search(q, top_k=5)


# PREPARE TRAINING DATA FOR LLM FINE-TUNING

from datasets import Dataset

# Generate Q&A pairs from document snippets
def generate_training_data(snippets, num_samples=100):
    """Create synthetic Q&A pairs for fine-tuning"""
    training_data = []

    # Sample diverse snippets
    sample_indices = np.random.choice(len(snippets), min(num_samples, len(snippets)), replace=False)

    for idx in sample_indices:
        context = snippets[idx]

        # Create instruction-following format
        training_data.append({
            "instruction": "Extract key information from the following legal clause:",
            "input": context,
            "output": context  # In real scenario, you'd have ground truth answers
        })

    return training_data

training_samples = generate_training_data(snippets, num_samples=100)
dataset = Dataset.from_list(training_samples)

print(f"Created {len(training_samples)} training samples")


# MODEL 1: FINE-TUNE DISTILGPT-2 

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import torch

print("\n" + "="*60)
print(" MODEL 1: Fine-tuning DistilGPT-2 (FREE, NO TOKEN REQUIRED)")
print("="*60)

model1_name = "distilgpt2"  # 100% free, no access token needed
tokenizer1 = AutoTokenizer.from_pretrained(model1_name)
tokenizer1.pad_token = tokenizer1.eos_token

model1 = AutoModelForCausalLM.from_pretrained(model1_name)

# Apply LoRA for efficient fine-tuning
lora_config1 = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["c_attn"]
)
model1 = get_peft_model(model1, lora_config1)

def tokenize_function(examples):
    texts = [f"### Instruction: {inst}\n### Input: {inp}\n### Output: {out}"
             for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output'])]
    return tokenizer1(texts, truncation=True, padding="max_length", max_length=256)

tokenized_dataset1 = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)

training_args1 = TrainingArguments(
    output_dir="./distilgpt2-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=50,
    logging_steps=10,
    learning_rate=2e-4,
    fp16=True,
    report_to="none",  # Disable wandb reporting
    save_total_limit=2,  # Keep only 2 checkpoints to save space
)

data_collator1 = DataCollatorForLanguageModeling(tokenizer=tokenizer1, mlm=False)

trainer1 = Trainer(
    model=model1,
    args=training_args1,
    train_dataset=tokenized_dataset1,
    data_collator=data_collator1,
)

trainer1.train()
print(" DistilGPT-2 fine-tuning completed!")

# MODEL 2: FINE-TUNE FLAN-T5-SMALL (FREE, NO TOKEN REQUIRED)

print("\n" + "="*60)
print(" MODEL 2: Fine-tuning FLAN-T5-Small (FREE, NO TOKEN REQUIRED)")
print("="*60)

from transformers import T5Tokenizer, T5ForConditionalGeneration

model2_name = "google/flan-t5-small"  # 100% free, no access token needed
tokenizer2 = T5Tokenizer.from_pretrained(model2_name)
model2 = T5ForConditionalGeneration.from_pretrained(model2_name)

lora_config2 = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q", "v"]
)
model2 = get_peft_model(model2, lora_config2)

def tokenize_t5(examples):
    inputs = [f"answer question: {inst} context: {inp}"
              for inst, inp in zip(examples['instruction'], examples['input'])]
    model_inputs = tokenizer2(inputs, max_length=256, truncation=True, padding="max_length")

    labels = tokenizer2(examples['output'], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset2 = dataset.map(tokenize_t5, batched=True, remove_columns=dataset.column_names)

training_args2 = TrainingArguments(
    output_dir="./flan-t5-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=50,
    logging_steps=10,
    report_to="none",  # Disable wandb reporting
    learning_rate=3e-4,

    fp16=True,
)

trainer2 = Trainer(
    model=model2,
    args=training_args2,
    train_dataset=tokenized_dataset2,
)

trainer2.train()
print("FLAN-T5-Small fine-tuning completed!")


# INFERENCE & COMPARISON (2 FINE-TUNED MODELS ONLY)

import time
import numpy as np

def generate_answer_gpt2(question, max_length=150):
    """Generate answer using fine-tuned DistilGPT-2"""
    prompt = f"### Instruction: Answer the following question\n### Input: {question}\n### Output:"
    inputs = tokenizer1(prompt, return_tensors="pt", truncation=True, max_length=256)

    # Move to GPU if available
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    outputs = model1.generate(
        **inputs,
        max_length=max_length,
        num_beams=4,
        early_stopping=True,
        temperature=0.7,
        do_sample=True
    )

    answer = tokenizer1.decode(outputs[0], skip_special_tokens=True)
    # Extract only the answer part
    if "### Output:" in answer:
        answer = answer.split("### Output:")[-1].strip()

    return answer

def generate_answer_t5(question, max_length=128):
    """Generate answer using fine-tuned FLAN-T5"""
    prompt = f"Answer the question: {question}"
    inputs = tokenizer2(prompt, return_tensors="pt", truncation=True, max_length=256)

    # Move to GPU if available
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    outputs = model2.generate(
        **inputs,
        max_length=max_length,
        num_beams=4,
        early_stopping=True,
        temperature=0.7,
        do_sample=True
    )

    answer = tokenizer2.decode(outputs[0], skip_special_tokens=True)
    return answer

def compare_models(question):
    """Compare answers from both fine-tuned models"""
    print("\n" + "="*80)
    print(f" QUESTION: {question}")
    print("="*80 + "\n")

    # Model 1: DistilGPT-2
    try:
        start = time.time()
        answer1 = generate_answer_gpt2(question)
        time1 = time.time() - start

        print(f"ðŸ¤– MODEL 1: DistilGPT-2 (Fine-tuned)")
        print(f"    Time: {time1:.3f}s")
        print(f"   Answer: {answer1}")
        print()

    except Exception as e:
        print(f"Error in DistilGPT-2: {e}\n")
        answer1 = "Error generating answer"
        time1 = 0

    # Model 2: FLAN-T5
    try:
        start = time.time()
        answer2 = generate_answer_t5(question)
        time2 = time.time() - start

        print(f" MODEL 2: FLAN-T5-Small (Fine-tuned)")
        print(f"     Time: {time2:.3f}s")
        print(f"    Answer: {answer2}")
        print()

    except Exception as e:
        print(f" Error in FLAN-T5: {e}\n")
        answer2 = "Error generating answer"
        time2 = 0

    print("-"*80)

    return {
        "question": question,
        "distilgpt2": {"answer": answer1, "time": time1, "length": len(answer1.split())},
        "flan_t5": {"answer": answer2, "time": time2, "length": len(answer2.split())},
    }


# TEST QUESTIONS

test_questions = [
    "Is there any limitation of liability?",
    "What is the warranty period?",
    "What happens if we terminate the contract?",
    "Are there financial penalties or fees?",
]

print("\n" + "="*80)
print("COMPARING FINE-TUNED MODELS")
print("="*80)
print(f"Testing {len(test_questions)} questions")
print(f"Models: DistilGPT-2 vs FLAN-T5-Small\n")

results = []
for i, q in enumerate(test_questions, 1):
    print(f"\n{'='*80}")
    print(f"Question {i}/{len(test_questions)}")
    print(f"{'='*80}")

    result = compare_models(q)

    if result is not None:
        results.append(result)
    else:
        print(f"Skipping question {i} due to errors")

print(f"\nSuccessfully processed {len(results)}/{len(test_questions)} questions")


# PERFORMANCE SUMMARY & COMPARISON

if results:
    print("\n" + "="*80)
    print(" COMPREHENSIVE PERFORMANCE SUMMARY")
    print("="*80 + "\n")

    # Calculate averages
    gpt2_times = [r["distilgpt2"]["time"] for r in results if r["distilgpt2"]["time"] > 0]
    t5_times = [r["flan_t5"]["time"] for r in results if r["flan_t5"]["time"] > 0]

    gpt2_lengths = [r["distilgpt2"]["length"] for r in results]
    t5_lengths = [r["flan_t5"]["length"] for r in results]

    # Time comparison
    print("  INFERENCE SPEED:")
    print(f"   DistilGPT-2:    {np.mean(gpt2_times):.3f}s average")
    print(f"   FLAN-T5-Small:  {np.mean(t5_times):.3f}s average")

    speed_winner = "DistilGPT-2" if np.mean(gpt2_times) < np.mean(t5_times) else "FLAN-T5-Small"
    print(f"   Fastest: {speed_winner}\n")

    # Answer length comparison
    print(" ANSWER LENGTH (words):")
    print(f"   DistilGPT-2:    {np.mean(gpt2_lengths):.1f} words average")
    print(f"   FLAN-T5-Small:  {np.mean(t5_lengths):.1f} words average\n")

    # Detailed comparison table
    print("DETAILED RESULTS:")
    print("-" * 80)
    print(f"{'Question':<50} {'GPT-2 Time':<15} {'T5 Time':<15}")
    print("-" * 80)

    for r in results:
        q_short = r["question"][:47] + "..." if len(r["question"]) > 50 else r["question"]
        gpt2_t = f"{r['distilgpt2']['time']:.3f}s"
        t5_t = f"{r['flan_t5']['time']:.3f}s"
        print(f"{q_short:<50} {gpt2_t:<15} {t5_t:<15}")

    print("-" * 80)

    # Quality metrics
    print("\n QUALITY OBSERVATIONS:")
    print(f"    DistilGPT-2 generated answers with avg {np.mean(gpt2_lengths):.0f} words")
    print(f"    FLAN-T5 generated answers with avg {np.mean(t5_lengths):.0f} words")

    if np.mean(gpt2_lengths) > np.mean(t5_lengths) * 1.5:
        print(f"    DistilGPT-2 tends to be more verbose")
    elif np.mean(t5_lengths) > np.mean(gpt2_lengths) * 1.5:
        print(f"    FLAN-T5 tends to be more verbose")
    else:
        print(f"    Both models produce similar length answers")

    print("\nModel comparison complete!")
    print("Tip: Review individual answers above to assess quality and relevance")

else:
    print("\nNo results to summarize - all queries failed")
    print("Make sure both models are properly fine-tuned before running comparison")

print("\n" + "="*80)
